{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad58e3c-d261-419d-8e6a-347bf5a8047b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:05:14.587628Z",
     "iopub.status.busy": "2024-03-13T15:05:14.586844Z",
     "iopub.status.idle": "2024-03-13T15:05:14.606776Z",
     "shell.execute_reply": "2024-03-13T15:05:14.605421Z",
     "shell.execute_reply.started": "2024-03-13T15:05:14.587578Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b185c29-bf09-4984-aa99-b534a3d68f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:05:14.697163Z",
     "iopub.status.busy": "2024-03-13T15:05:14.696107Z",
     "iopub.status.idle": "2024-03-13T15:05:16.926291Z",
     "shell.execute_reply": "2024-03-13T15:05:16.925062Z",
     "shell.execute_reply.started": "2024-03-13T15:05:14.697091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, re\n",
    "import torch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "rng = np.random.default_rng(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"../data/raw/AFDBv4_90.128-254.fasta\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    sequences = [seq.strip() for seq in lines if not seq.startswith(\">\")]\n",
    "    df = pd.DataFrame({'sequence':sequences})\n",
    "train_data, test_data = train_test_split(df, test_size=0.5, shuffle = True, random_state=42)\n",
    "test_data, val_data = train_test_split(test_data, test_size=0.5, shuffle = True, random_state=42)\n",
    "train_data = pd.read_csv('data/train.txt', names=['sequence'])\n",
    "test_data = pd.read_csv('data/test.txt', names=['sequence'])\n",
    "val_data = pd.read_csv('data/val.txt', names=['sequence'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247122ec-f047-4c75-8edb-bd954f2d1712",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:05:16.928685Z",
     "iopub.status.busy": "2024-03-13T15:05:16.927833Z",
     "iopub.status.idle": "2024-03-13T15:05:17.482243Z",
     "shell.execute_reply": "2024-03-13T15:05:17.481146Z",
     "shell.execute_reply.started": "2024-03-13T15:05:16.928630Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('data/val.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    sequences = [seq.strip() for seq in lines if not seq.startswith(\">\")]\n",
    "    val_data_df = pd.DataFrame({'sequence':sequences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73a9e200-9e37-4a95-89fc-89841a47fa5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:05:17.484904Z",
     "iopub.status.busy": "2024-03-13T15:05:17.484270Z",
     "iopub.status.idle": "2024-03-13T15:06:31.527565Z",
     "shell.execute_reply": "2024-03-13T15:06:31.526140Z",
     "shell.execute_reply.started": "2024-03-13T15:05:17.484854Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        n = len(line)\n",
    "\n",
    "        idx1, idx2 = torch.randperm(n-2)[:2] + 1\n",
    "        if idx1 > idx2:\n",
    "            idx1, idx2 = idx2, idx1\n",
    "\n",
    "        prefix, middle, suffix = line[:idx1], line[idx1:idx2], line[idx2:]\n",
    "        \n",
    "        p = rng.random()\n",
    "        if p > 0.66: # PSM\n",
    "            fim_sample = '@' + prefix + '$' + suffix + '#' + middle\n",
    "        elif p > 0.33: # SPM\n",
    "            fim_sample = '$' + suffix + '@' + prefix + '#' + middle\n",
    "        else: # default\n",
    "            fim_sample = prefix + middle + suffix\n",
    "\n",
    "        output.append(fim_sample)\n",
    "\n",
    "    return '.'.join(output)\n",
    "\n",
    "train_data = process_file('data/train.txt')\n",
    "val_data = process_file('data/val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0b437d1-2de3-404b-a0c8-715c2a3b918c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:31.529832Z",
     "iopub.status.busy": "2024-03-13T15:06:31.529259Z",
     "iopub.status.idle": "2024-03-13T15:06:38.197797Z",
     "shell.execute_reply": "2024-03-13T15:06:38.196390Z",
     "shell.execute_reply.started": "2024-03-13T15:06:31.529800Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4,  5,  6,  0, 21, 10,  1,  2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(list(set(\"\".join(train_data))) + ['0']) #<PRE> = '@', <MID> = '#', <SUF> = '$', <EOS> = '.', <PAD> = '0'\n",
    "\n",
    "stoi = {c:i for i,c in enumerate(vocab)}\n",
    "itos = {i:c for i,c in enumerate(vocab)}\n",
    "\n",
    "encode = lambda s: torch.LongTensor([stoi[c] for c in s])\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "encode(\"@AC#TG$.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab759c6d-4393-4859-b2c6-e903e5e9d196",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.201164Z",
     "iopub.status.busy": "2024-03-13T15:06:38.199917Z",
     "iopub.status.idle": "2024-03-13T15:06:38.221136Z",
     "shell.execute_reply": "2024-03-13T15:06:38.219978Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.201112Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_val_batch(device=\"cuda\", mode='psm', indexes=None):\n",
    "    data = val_data_df\n",
    "    data = data.sequence.values\n",
    "    \n",
    "    ix = rng.integers(len(data), size=batch_size)\n",
    "    xs1 = []\n",
    "    ys1 = []\n",
    "    xs2 = []\n",
    "    ys2 = []\n",
    "    xs3 = []\n",
    "    ys3 = []\n",
    "\n",
    "    mask = []\n",
    "    for i in ix:\n",
    "        # if mode == 'psm':\n",
    "        document = data[i][:ctx_size]\n",
    "        n = len(document)\n",
    "        idx1, idx2 = torch.randperm(n-2)[:2] + 1\n",
    "        if idx1 > idx2:\n",
    "            idx1, idx2 = idx2, idx1\n",
    "\n",
    "        prefix, middle, suffix = document[:idx1], document[idx1:idx2], document[idx2:]\n",
    "        fim_sample = '@' + prefix + '$' + suffix + '#' + middle\n",
    "\n",
    "        sample_x1 = encode(fim_sample[:ctx_size])\n",
    "        sample_y1 = encode(fim_sample[1:ctx_size+1])\n",
    "\n",
    "        # if mode == 'pms':\n",
    "        # document = data[i][:ctx_size]\n",
    "        # n = len(document)\n",
    "        # idx1, idx2 = torch.randperm(n-2)[:2] + 1\n",
    "        # prefix, middle, _ = document[:idx1], document[idx1:idx2], document[idx2:]\n",
    "        fim_sample = prefix + middle\n",
    "\n",
    "        sample_x2 = encode(fim_sample[:ctx_size])\n",
    "        sample_y2 = encode(fim_sample[1:ctx_size+1])\n",
    "\n",
    "        mask.append(torch.tensor(len(prefix)))\n",
    "\n",
    "        # elif mode == 'default':\n",
    "        sample_x3 = encode(data[i][:ctx_size])\n",
    "        sample_y3 = encode(data[i][1:ctx_size+1])\n",
    "\n",
    "        sample_x1 = F.pad(sample_x1, (0, max(0, ctx_size - len(sample_x1))), value=stoi['0'])\n",
    "        sample_y1 = F.pad(sample_y1, (0, max(0, ctx_size - len(sample_y1))), value=stoi['0'])\n",
    "        \n",
    "        sample_x2 = F.pad(sample_x2, (0, max(0, ctx_size - len(sample_x2))), value=stoi['0'])\n",
    "        sample_y2 = F.pad(sample_y2, (0, max(0, ctx_size - len(sample_y2))), value=stoi['0'])\n",
    "        \n",
    "        sample_x3 = F.pad(sample_x3, (0, max(0, ctx_size - len(sample_x3))), value=stoi['0'])\n",
    "        sample_y3 = F.pad(sample_y3, (0, max(0, ctx_size - len(sample_y3))), value=stoi['0'])\n",
    "\n",
    "\n",
    "        xs1.append(sample_x1)\n",
    "        ys1.append(sample_y1)\n",
    "        \n",
    "        xs2.append(sample_x2)\n",
    "        ys2.append(sample_y2)\n",
    "\n",
    "        xs3.append(sample_x3)\n",
    "        ys3.append(sample_y3)\n",
    "\n",
    "    x1 = torch.stack(xs1).to(device)\n",
    "    y1 = torch.stack(ys1).to(device)\n",
    "\n",
    "    x2 = torch.stack(xs2).to(device)\n",
    "    y2 = torch.stack(ys2).to(device)\n",
    "\n",
    "    x3 = torch.stack(xs3).to(device)\n",
    "    y3 = torch.stack(ys3).to(device)\n",
    "\n",
    "    mask = torch.stack(mask).to(device)\n",
    "    return x1, x2, x3, y1, y2, y3, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd0391d0-d0ad-4480-96a6-23ca2a4b8015",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.222934Z",
     "iopub.status.busy": "2024-03-13T15:06:38.222219Z",
     "iopub.status.idle": "2024-03-13T15:06:38.237392Z",
     "shell.execute_reply": "2024-03-13T15:06:38.236301Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.222886Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "ctx_size = 512\n",
    "\n",
    "def get_batch(split, device=\"cuda\"):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = rng.integers(len(data) - ctx_size, size=batch_size)\n",
    "    x = torch.stack([encode(data[i:i+ctx_size]) for i in ix])\n",
    "    y = torch.stack([encode(data[i+1:i+ctx_size+1]) for i in ix])\n",
    "    return x.to(device), y.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82f0be09-a2b1-4f81-94fd-f5b7a0b8f5c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.239563Z",
     "iopub.status.busy": "2024-03-13T15:06:38.238593Z",
     "iopub.status.idle": "2024-03-13T15:06:38.357819Z",
     "shell.execute_reply": "2024-03-13T15:06:38.356514Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.239509Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: torch.Size([512, 512])\n",
      "targets: torch.Size([512, 512])\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train', 'cpu')\n",
    "print('inputs:', xb.shape)\n",
    "# print(xb)\n",
    "print('targets:', yb.shape)\n",
    "# print(yb)\n",
    "\n",
    "# for t in range(ctx_size):\n",
    "#     context = xb[0,:t+1]\n",
    "#     target = yb[0,t]\n",
    "#     print(f\"when input is {decode(context.numpy())} the target: {itos[int(target.numpy())]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "381e29fe-9900-4d97-b8fc-00e3e73fea8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.362331Z",
     "iopub.status.busy": "2024-03-13T15:06:38.361683Z",
     "iopub.status.idle": "2024-03-13T15:06:38.376995Z",
     "shell.execute_reply": "2024-03-13T15:06:38.375891Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.362283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# xb, yb, _ = get_val_batch(device=device, mode='psm')\n",
    "# print('inputs:', xb.shape)\n",
    "# # print(xb)\n",
    "# print('targets:', yb.shape)\n",
    "# for t in range(ctx_size):\n",
    "#     context = xb[1,:t+1]\n",
    "#     target = yb[1,t]\n",
    "#     print(f\"when input is {decode(context.cpu().numpy())} the target: {itos[int(target.cpu().numpy())]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ca4edb-5dbe-411b-995b-5f6f430a107a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.380240Z",
     "iopub.status.busy": "2024-03-13T15:06:38.379077Z",
     "iopub.status.idle": "2024-03-13T15:06:38.401053Z",
     "shell.execute_reply": "2024-03-13T15:06:38.399992Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.380184Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, embed_size, head_embed_size, dropout=0):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(embed_size, head_embed_size, bias=False)\n",
    "        self.query = nn.Linear(embed_size, head_embed_size, bias=False)\n",
    "        self.value = nn.Linear(embed_size, head_embed_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones((ctx_size, ctx_size))))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        _,T,_ = inputs.shape # batch, ctx, vocab_size\n",
    "        k = self.key(inputs) # batch, ctx, head_size\n",
    "        q = self.query(inputs) # batch, ctx, head_size\n",
    "        mask = q @ k.transpose(-2, -1) # batch, ctx, ctx\n",
    "        mask = mask.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\"))\n",
    "        mask = F.softmax(mask, dim=-1)\n",
    "        mask = self.dropout(mask)\n",
    "        v = self.value(inputs) # batch, ctx, head_size\n",
    "        out = mask @ v # batch, ctx, head_size\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, dropout=0):\n",
    "        super().__init__()\n",
    "        head_embed_size = embed_size // num_heads\n",
    "        self.heads = nn.ModuleList([Head(embed_size, head_embed_size, dropout) for _ in range(num_heads)])\n",
    "        self.ff = nn.Linear(head_embed_size*num_heads, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # batch, ctx, embed_size\n",
    "        inputs = torch.cat([head(inputs) for head in self.heads], dim=-1) # batch, ctx, num_head*head_size --> batch, ctx, embed_size\n",
    "        inputs = self.ff(inputs) # batch, ctx, embed_size\n",
    "        inputs = self.dropout(inputs)\n",
    "        return inputs\n",
    "    \n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, num_heads, embed_size, dropout=0):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_size)\n",
    "        self.mha = MultiHeadAttention(num_heads, embed_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.mha(self.ln1(inputs)) + inputs # batch, ctx, embed_size\n",
    "        inputs = self.ff(self.ln2(inputs)) + inputs  # batch, ctx, embed_size\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e038384-a2b9-42f2-b18b-2ed3f2299f59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.403312Z",
     "iopub.status.busy": "2024-03-13T15:06:38.402172Z",
     "iopub.status.idle": "2024-03-13T15:06:38.417828Z",
     "shell.execute_reply": "2024-03-13T15:06:38.416846Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.403248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e250457-8841-4213-9097-3a422edd3d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.419709Z",
     "iopub.status.busy": "2024-03-13T15:06:38.418992Z",
     "iopub.status.idle": "2024-03-13T15:06:38.530311Z",
     "shell.execute_reply": "2024-03-13T15:06:38.528999Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.419669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embed_size = 512\n",
    "num_heads = 3\n",
    "num_layers = 3\n",
    "dropout = 0.0\n",
    "lr = 3e-5\n",
    "steps = 2001\n",
    "evel_interv = 250\n",
    "generate_interv = 1000\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "VOCAB_SIZE = len(vocab)\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, embed_size)\n",
    "        self.blocks = nn.Sequential(*[Block(num_heads, embed_size) for _ in range(num_layers)])\n",
    "        self.ln = nn.LayerNorm(embed_size)\n",
    "        self.ff = nn.Linear(embed_size, VOCAB_SIZE)\n",
    "    \n",
    "    def forward(self, inputs, targets=None):\n",
    "        # batch, ctx, vocab_size\n",
    "        logits = self.embedding(inputs) # batch, ctx, embed_size\n",
    "        logits = self.blocks(logits) # batch, ctx, embed_size\n",
    "        logits = self.ln(logits) # batch, ctx, embed_size\n",
    "        logits = self.ff(logits) # batch, ctx, vocab_size\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            lv = logits.view((B*T, C))\n",
    "            tv = targets.view((B*T,))\n",
    "            loss = F.cross_entropy(lv, tv)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, tokens, n=256):\n",
    "        for _ in range(n):\n",
    "            logits, loss = self(tokens[:,-ctx_size:])\n",
    "            logits = logits[:,-1,:]\n",
    "            logits = F.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(logits, num_samples=1)\n",
    "            if decode(next_token[0].cpu().numpy()) == '.':\n",
    "                break\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "        return tokens\n",
    "\n",
    "    \n",
    "    def calculate_loss(self, batch_tokens, targets, mode='psm', indexes=None):\n",
    "        if mode == 'psm':\n",
    "            mask = torch.zeros_like(batch_tokens).to(device)\n",
    "\n",
    "            hash_index = torch.tensor(stoi['#']).to(device)\n",
    "            indices = (batch_tokens == hash_index).nonzero()[:,1] + 1\n",
    "            for b, idx in enumerate(indices):\n",
    "                mask[b, idx:] = 1\n",
    "                \n",
    "            pad_index = torch.tensor(stoi['0']).to(device)\n",
    "            mask = (mask & (batch_tokens != pad_index)).to(torch.long)\n",
    "            \n",
    "            logits, _ = self(batch_tokens)\n",
    "            logits = logits[:,-batch_tokens.shape[1] + 1:,:]\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), targets[:,1:], reduction='none')\n",
    "            loss = (loss * mask[:,1:]).sum() / mask.sum()\n",
    "\n",
    "        elif mode == 'pms':\n",
    "            mask = torch.zeros_like(batch_tokens).to(device)\n",
    "\n",
    "            for b, idx in enumerate(indexes):\n",
    "                mask[b, idx:] = 1\n",
    "            \n",
    "            pad_index = torch.tensor(stoi['0']).to(device)\n",
    "            mask = (mask & (batch_tokens != pad_index)).to(torch.long)\n",
    "\n",
    "\n",
    "            logits, _ = self(batch_tokens)\n",
    "            logits = logits[:, -batch_tokens.shape[1] + 1:, :]\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), targets[:, 1:], reduction='none')\n",
    "\n",
    "            loss = (loss * mask[:, 1:]).sum() / mask.sum()\n",
    "            \n",
    "        elif mode == 'default':\n",
    "            pad_index = torch.tensor(stoi['0']).to(device)\n",
    "            mask = (batch_tokens != pad_index).to(torch.long)\n",
    "            logits, _ = self(batch_tokens)\n",
    "            logits = logits[:,-batch_tokens.shape[1] + 1:,:]\n",
    "            loss = F.cross_entropy(logits.permute(0, 2, 1), targets[:,1:], reduction='none')\n",
    "            loss = (loss * mask[:,1:]).sum() / mask.sum()\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1488ece3-5839-4a41-8afa-791fe9ea5d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.532404Z",
     "iopub.status.busy": "2024-03-13T15:06:38.531761Z",
     "iopub.status.idle": "2024-03-13T15:06:38.547311Z",
     "shell.execute_reply": "2024-03-13T15:06:38.546232Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.532367Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5286f4-7ad2-491c-9021-36111b20d4e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.549292Z",
     "iopub.status.busy": "2024-03-13T15:06:38.548461Z",
     "iopub.status.idle": "2024-03-13T15:06:38.934526Z",
     "shell.execute_reply": "2024-03-13T15:06:38.933439Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.549235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageModel()\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe30fc4-4ae0-4b87-9950-8d1f02603d21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:06:38.936514Z",
     "iopub.status.busy": "2024-03-13T15:06:38.935751Z",
     "iopub.status.idle": "2024-03-13T15:46:53.071826Z",
     "shell.execute_reply": "2024-03-13T15:46:53.070393Z",
     "shell.execute_reply.started": "2024-03-13T15:06:38.936457Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.743705 M parameters\n",
      "loss: 3.355654239654541\n",
      "$H#FP$GAHWVDQCNMAMNID\n",
      "batch_size=512 ctx_size=512 embed_size=512 num_heads=3 num_layers=3 dropout=0.0\n",
      "Step    0: train loss 3.35601 | val loss 3.35695 | AR loss 3.342895030975342 | AR middle loss 3.3614940643310547 | FIM loss 3.340390682220459 | FIM middle loss 3.3592135906219482\n",
      "Step  100: train loss 2.90756 | val loss 2.90893 | AR loss 2.899994134902954 | AR middle loss 2.9614362716674805 | FIM loss 2.9317800998687744 | FIM middle loss 2.960427761077881\n",
      "Step  200: train loss 2.90424 | val loss 2.90392 | AR loss 2.894641876220703 | AR middle loss 2.9637420177459717 | FIM loss 2.9269349575042725 | FIM middle loss 2.9620444774627686\n",
      "Step  300: train loss 2.90444 | val loss 2.90104 | AR loss 2.8839876651763916 | AR middle loss 2.9621920585632324 | FIM loss 2.916936159133911 | FIM middle loss 2.961125135421753\n",
      "Step  400: train loss 2.90234 | val loss 2.90045 | AR loss 2.8943653106689453 | AR middle loss 2.9754397869110107 | FIM loss 2.9265992641448975 | FIM middle loss 2.9739725589752197\n",
      "Step  500: train loss 2.90024 | val loss 2.89978 | AR loss 2.8898890018463135 | AR middle loss 2.983076572418213 | FIM loss 2.9223713874816895 | FIM middle loss 2.981501579284668\n",
      "Step  600: train loss 2.89555 | val loss 2.89624 | AR loss 2.8853211402893066 | AR middle loss 2.9741697311401367 | FIM loss 2.917591094970703 | FIM middle loss 2.972761392593384\n",
      "Step  700: train loss 2.89195 | val loss 2.89321 | AR loss 2.89485502243042 | AR middle loss 2.987109899520874 | FIM loss 2.926862955093384 | FIM middle loss 2.985187530517578\n",
      "Step  800: train loss 2.89951 | val loss 2.89285 | AR loss 2.8903279304504395 | AR middle loss 2.9702870845794678 | FIM loss 2.9215176105499268 | FIM middle loss 2.968501091003418\n",
      "Step  900: train loss 2.89598 | val loss 2.89317 | AR loss 2.8935179710388184 | AR middle loss 2.9865739345550537 | FIM loss 2.9259419441223145 | FIM middle loss 2.9853484630584717\n",
      "Step 1000: train loss 2.89151 | val loss 2.89391 | AR loss 2.8937437534332275 | AR middle loss 2.9978153705596924 | FIM loss 2.9265100955963135 | FIM middle loss 2.997117280960083\n",
      "Step 1100: train loss 2.89174 | val loss 2.89649 | AR loss 2.8896644115448 | AR middle loss 2.984405279159546 | FIM loss 2.922642946243286 | FIM middle loss 2.9836409091949463\n",
      "Step 1200: train loss 2.89526 | val loss 2.89565 | AR loss 2.8931467533111572 | AR middle loss 2.9905450344085693 | FIM loss 2.926133871078491 | FIM middle loss 2.989652395248413\n",
      "Step 1300: train loss 2.89141 | val loss 2.89063 | AR loss 2.8986310958862305 | AR middle loss 2.998621702194214 | FIM loss 2.9315831661224365 | FIM middle loss 2.9972548484802246\n",
      "Step 1400: train loss 2.89000 | val loss 2.88870 | AR loss 2.8982181549072266 | AR middle loss 2.991128921508789 | FIM loss 2.9300358295440674 | FIM middle loss 2.9910366535186768\n",
      "Step 1500: train loss 2.89151 | val loss 2.88967 | AR loss 2.8899266719818115 | AR middle loss 2.983367919921875 | FIM loss 2.9212968349456787 | FIM middle loss 2.9842159748077393\n",
      "Step 1600: train loss 2.88983 | val loss 2.88898 | AR loss 2.8961286544799805 | AR middle loss 2.9942824840545654 | FIM loss 2.926889419555664 | FIM middle loss 2.9955344200134277\n",
      "Step 1700: train loss 2.88551 | val loss 2.88939 | AR loss 2.888540506362915 | AR middle loss 2.988342523574829 | FIM loss 2.9202797412872314 | FIM middle loss 2.9934639930725098\n",
      "Step 1800: train loss 2.88819 | val loss 2.88726 | AR loss 2.8823564052581787 | AR middle loss 2.9781394004821777 | FIM loss 2.912325382232666 | FIM middle loss 2.9796254634857178\n",
      "Step 1900: train loss 2.88777 | val loss 2.88726 | AR loss 2.8907852172851562 | AR middle loss 2.989698886871338 | FIM loss 2.9198575019836426 | FIM middle loss 2.990445613861084\n",
      "Step 2000: train loss 2.88400 | val loss 2.88680 | AR loss 2.892845869064331 | AR middle loss 2.987222671508789 | FIM loss 2.9233152866363525 | FIM middle loss 2.9916253089904785\n",
      "\n",
      "2.8825178146362305\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel()\n",
    "model.to(device)\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
    "xb, yb = get_batch(\"train\", device)\n",
    "logits, loss = model(xb, yb)\n",
    "print(\"loss:\", loss.item())\n",
    "\n",
    "print(decode(model.generate(torch.ones((1,1), dtype=torch.int64).to(device), 64)[0].cpu().numpy()))\n",
    "\n",
    "\n",
    "print(f\"batch_size={batch_size} ctx_size={ctx_size} embed_size={embed_size} num_heads={num_heads} num_layers={num_layers} dropout={dropout}\")\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for step in range(steps):\n",
    "    xb, yb = get_batch('train', device)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            splits = {}\n",
    "            for split in ['train', 'val']:\n",
    "                losses = torch.zeros(1)\n",
    "                for k in range(1):\n",
    "                    X, y = get_batch(split, device)\n",
    "                    logits, loss = model(X, y)\n",
    "                    losses[k] = loss.item()\n",
    "                splits[split] = losses.mean()\n",
    "                        \n",
    "            x1, x2, x3, y1, y2, y3, ixs = get_val_batch(device=device)\n",
    "            loss_1 = model.calculate_loss(x3, y3, mode='default')\n",
    "            \n",
    "            loss_2 = model.calculate_loss(x2, y2, mode='pms', indexes=ixs)\n",
    "            \n",
    "            loss_3 = model.calculate_loss(x1, y1, mode='default')\n",
    "\n",
    "            loss_4 = model.calculate_loss(x1, y1, mode='psm')\n",
    "\n",
    "            print(f\"Step {step:4}: train loss {splits['train']:.5f} | val loss {splits['val']:.5f}\"\n",
    "             f\" | AR loss {loss_1} | AR middle loss {loss_2} | FIM loss {loss_3} | FIM middle loss {loss_4}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    _, loss = model(xb, yb)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "print()\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602e5865-512f-4ffe-95b1-95d562c3fd2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T15:50:55.068144Z",
     "iopub.status.busy": "2024-03-13T15:50:55.067103Z",
     "iopub.status.idle": "2024-03-13T15:50:55.271805Z",
     "shell.execute_reply": "2024-03-13T15:50:55.270786Z",
     "shell.execute_reply.started": "2024-03-13T15:50:55.068079Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"./gpt_5M.pth\"\n",
    "torch.save(model, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad09b17-68b8-4374-b3cb-1cd6701b368c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T16:26:02.673367Z",
     "iopub.status.busy": "2024-03-13T16:26:02.672572Z",
     "iopub.status.idle": "2024-03-13T17:06:15.611148Z",
     "shell.execute_reply": "2024-03-13T17:06:15.610072Z",
     "shell.execute_reply.started": "2024-03-13T16:26:02.673317Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Step    0: loss 3.41977\n",
      "Step  100: loss 2.90874\n",
      "Step  200: loss 2.90608\n",
      "Step  300: loss 2.90013\n",
      "Step  400: loss 2.90002\n",
      "Step  500: loss 2.89754\n",
      "Step  600: loss 2.90122\n",
      "Step  700: loss 2.89556\n",
      "Step  800: loss 2.89644\n",
      "Step  900: loss 2.90114\n",
      "Step 1000: loss 2.89701\n",
      "Step 1100: loss 2.89349\n",
      "Step 1200: loss 2.89301\n",
      "Step 1300: loss 2.89157\n",
      "Step 1400: loss 2.89195\n",
      "Step 1500: loss 2.89415\n",
      "Step 1600: loss 2.89119\n",
      "Step 1700: loss 2.88981\n",
      "Step 1800: loss 2.88975\n",
      "Step 1900: loss 2.89163\n",
      "Step 2000: loss 2.89196\n",
      "\n",
      "Final loss: 2.891958713531494\n"
     ]
    }
   ],
   "source": [
    "!python3 train_ddp.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
